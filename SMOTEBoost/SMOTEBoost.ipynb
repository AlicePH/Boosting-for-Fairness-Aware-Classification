{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The implementation of SMOTEBoost.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Simona Nitti, Gabriel Rozzonelli\n",
    "# Based on the work of the following paper:\n",
    "# [1] N. Chawla, A. Lazarevic, L. Hall, et K. Bowyer, « SMOTEBoost: \n",
    "#     Improving Prediction  of the Minority Class in Boosting ».\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.base import is_classifier, clone\n",
    "from sklearn.utils.multiclass import check_classification_targets, type_of_target\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTE:\n",
    "    \"\"\"SMOTE\n",
    "    \n",
    "    Performs SMOTE resampling to address class imbalance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k_neighbors : int, default=5\n",
    "        The number of nearest neighbors.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    k_neighbors_ : int\n",
    "        The number of nearest neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k_neighbors=5):\n",
    "        self.k_neighbors_ = k_neighbors\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit SMOTE on a training set, by looking for the `k_neighbors`\n",
    "        nearest neighbors of each sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "           The samples to oversample from.\n",
    "        \"\"\"\n",
    "        self.X = check_array(X)\n",
    "        self.n_features_in_ = self.X.shape[1]\n",
    "\n",
    "        # Fit nearest neighbors\n",
    "        n_neighbors = self.k_neighbors_ + 1\n",
    "        self.neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        self.neigh.fit(self.X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"\n",
    "        Generate new synthetic samples from the training samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            The number of new synthetic samples to generate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like of shape (n_samples, n_features)\n",
    "            The new synthetic samples.\n",
    "        \"\"\"\n",
    "        X_new = np.zeros((n_samples, self.n_features_in_))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            \n",
    "            # Pick a sample randomly\n",
    "            j = np.random.choice(range(self.X.shape[0]))\n",
    "\n",
    "            # Take the k nearest neighbors around it\n",
    "            X_j = self.X[j].reshape(1, -1)\n",
    "            new_neighs = self.neigh.kneighbors(X_j, return_distance=False)\n",
    "            \n",
    "            # Keep all columns but the first one as it is X[j] itself\n",
    "            new_neighs = new_neighs[:,1:]\n",
    "            \n",
    "            # Choose one of the k neighbors\n",
    "            new_neigh_index = np.random.choice(new_neighs[0])  \n",
    "            \n",
    "            # Measure the index between X[j] and the randomly chosen neighbor\n",
    "            distance = self.X[new_neigh_index] - self.X[j] \n",
    "            fraction = np.random.random()\n",
    "            \n",
    "            # Synthetize a new sample\n",
    "            X_new[i] = self.X[j] + fraction * distance\n",
    "\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTEBoostClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"SMOTEBoost Classifier (SMOTEBoostClassifier)\n",
    "    \n",
    "    Parameters\n",
    "    ----------        \n",
    "    base_classifier : object\n",
    "        Base classifier from which the boosted ensemble is built.\n",
    "        \n",
    "    n_estimators : int, default=3\n",
    "        The number of base estimators.\n",
    "        \n",
    "    k_neighbors : int, default=5\n",
    "        Number of nearest neighbors for SMOTE.\n",
    "        \n",
    "    n : int, default=5\n",
    "        The number of new synthetic samples per boost iteration.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    classifiers_ : list\n",
    "        The collection of fitted base classifiers.\n",
    "\n",
    "    classes_ : array of shape (n_classes,)\n",
    "        The classes labels.\n",
    "    \n",
    "    minority_class_ : int\n",
    "        Class identified as the minority class.\n",
    "\n",
    "    alphas_ : array of shape (n_estimators,)\n",
    "        The weights for each estimator in the boosted ensemble.\n",
    "    \"\"\"\n",
    "    \n",
    "    _required_parameters = [\"base_classifier\"]\n",
    "\n",
    "    def __init__(self, base_classifier, n_estimators=3, k_neighbors=5, n=5): \n",
    "        self.base_classifier = base_classifier  \n",
    "        self.n_estimators = n_estimators  \n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.n = n\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build a SMOTEBoost classifier from the training set (X, y).\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "           The training input samples.\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "           The target values (class labels) as integers.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self: object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        # Check that the base estimator is a classifier\n",
    "        if not is_classifier(self.base_classifier):\n",
    "            raise ValueError(\"The base estimator should be a classifier, \" \\\n",
    "                             \"but it is not.\")\n",
    "                             \n",
    "        # Check that the it is a binary classification problem\n",
    "        check_classification_targets(y)\n",
    "        n_classes = len(np.unique(y))\n",
    "        if type_of_target(y) != \"binary\":\n",
    "            raise ValueError(\"Target values should belong to a binary set, \" \\\n",
    "                             \"but {} classes were found.\".format(n_classes))\n",
    "        \n",
    "        # Initialize lists to hold models and model weights\n",
    "        self.classifiers_ = []\n",
    "        self.alphas_ = []\n",
    "\n",
    "        # Find the minority class\n",
    "        self.classes_, counts = np.unique(y, return_counts=True)\n",
    "        self.minority_class_ = self.classes_[counts==-np.max(-counts)][0]\n",
    "        X_minority = X[y == self.minority_class_]\n",
    "        \n",
    "        # Fit SMOTE on the sensitive samples       \n",
    "        smote = SMOTE(k_neighbors=self.k_neighbors)\n",
    "        smote.fit(X_minority)\n",
    "\n",
    "        # Initialize the distribution\n",
    "        dist = np.ones_like(X) / (len(y) * (n_classes-1))  \n",
    "        for i in range(len(y)):\n",
    "            dist[i, np.where(self.classes_ == y[i])[0][0]] = 0\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Create new artificial samples from the minority class\n",
    "            X_new = smote.sample(self.n)\n",
    "            y_new = np.ones(self.n) * self.minority_class_\n",
    "\n",
    "            # Append the new examples\n",
    "            X_smote = np.concatenate((X, X_new))\n",
    "            y_smote = np.concatenate((y, y_new))\n",
    "\n",
    "            # Train a weak learner with the modified distribution\n",
    "            self.classifiers_.append(clone(self.base_classifier))     \n",
    "            self.classifiers_[i].fit(X_smote, y_smote)\n",
    "\n",
    "            # Make predictions over the initial dataset\n",
    "            h = self.classifiers_[i].predict_proba(X) \n",
    "\n",
    "            # Compute the pseudo-loss of hypothesis\n",
    "            # TODO: Try to avoid nested loops\n",
    "            epsilon = 0\n",
    "            for k in range(len(y)):\n",
    "                for j in range(n_classes):\n",
    "                    epsilon += dist[k,j] \\\n",
    "                    * (1.- h[k,np.where(self.classes_ == y[k])[0][0]] + h[k,j])\n",
    "            beta = epsilon / (1. - epsilon)\n",
    "            self.alphas_.append(np.log(1/beta))\n",
    "\n",
    "            # Update distribution\n",
    "            # TODO: Try to avoid nested loops\n",
    "            z = np.sum(dist)\n",
    "            for k in range(len(y)):\n",
    "                for j in range(n_classes):\n",
    "                    exp = (1. + h[k,np.where(self.classes_==y[k])[0][0]] - h[k,j]) / 2\n",
    "                    dist[k,j] = dist[k,j] / z * beta**exp\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for X.\n",
    "        \n",
    "        The predicted class of an input sample is computed according to\n",
    "        the base estimator that underwent the optimization procedure.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        \n",
    "        X = check_array(X)\n",
    "        \n",
    "        final_predictions = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        y = np.zeros(X.shape[0])\n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            final_predictions \\\n",
    "                += self.classifiers_[t].predict_proba(X) * self.alphas_[t]\n",
    "            \n",
    "                \n",
    "        for i in range(len(X)):\n",
    "            try:\n",
    "                y[i] = self.classes_[final_predictions[i,:] == np.amax(final_predictions[i,:])][0]\n",
    "            except (IndexError):\n",
    "                raise Exception('Increase number of observaitions')\n",
    "                \n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def _more_tags(self):\n",
    "        tags = {\n",
    "            \"binary_only\": True,\n",
    "        }\n",
    "        \n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=0)\n",
    "    \n",
    "stamp = DecisionTreeClassifier(max_depth=1)\n",
    "smote = SMOTEBoostClassifier(stamp)\n",
    "smote.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicitons = smote.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.23      0.22       500\n",
      "           1       0.15      0.13      0.14       500\n",
      "\n",
      "    accuracy                           0.18      1000\n",
      "   macro avg       0.18      0.18      0.18      1000\n",
      "weighted avg       0.18      0.18      0.18      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, predicitons))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
